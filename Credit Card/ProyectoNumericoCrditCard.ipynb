{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az9dUERvQjwn"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle # Import pickle here as it's used later\n",
        "import seaborn as sns # Import seaborn explicitly\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc # Import metrics here\n",
        "\n",
        "# Cargar el dataset\n",
        "data = pd.read_csv('UCI_Credit_Card.csv')\n",
        "\n",
        "# Exploración inicial\n",
        "print(data.head())\n",
        "print(data.info())\n",
        "print(data.describe())\n",
        "\n",
        "# Verificar valores nulos\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Eliminar duplicados si existen\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "# Separar características y variable objetivo\n",
        "# X originally has 23 features ('ID' dropped)\n",
        "X = data.drop(['default.payment.next.month', 'ID'], axis=1) # Ensure 'ID' is dropped here\n",
        "y = data['default.payment.next.month']\n",
        "\n",
        "# Codificar variables categóricas si las hay (should not be any in this dataset)\n",
        "# Although not strictly needed for this dataset, keep the code for generality\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "if len(categorical_cols) > 0:\n",
        "    X = pd.get_dummies(X, columns=categorical_cols)\n",
        "\n",
        "# Dividir en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Estandarizar las características\n",
        "# IMPORTANT: Fit scaler *before* adding the intercept\n",
        "scaler = StandardScaler()\n",
        "print(f\"Shape of X_train before fitting scaler: {X_train.shape}\") # Debug print\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Añadir término de intercepción (bias) *after* scaling\n",
        "X_train = np.hstack([np.ones((X_train_scaled.shape[0], 1)), X_train_scaled])\n",
        "X_test = np.hstack([np.ones((X_test_scaled.shape[0], 1)), X_test_scaled])\n",
        "\n",
        "# Verify shapes after adding intercept\n",
        "print(f\"Shape of X_train after adding intercept: {X_train.shape}\") # Debug print\n",
        "print(f\"Shape of X_test after adding intercept: {X_test.shape}\")   # Debug print\n",
        "\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-4):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.weights = None\n",
        "        self.loss_history = [] # Store loss for each fit\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        # Handle potential overflow/underflow in sigmoid\n",
        "        z = np.clip(z, -500, 500) # Clip values to prevent exp overflow\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def compute_loss(self, X, y, weights):\n",
        "        m = X.shape[0]\n",
        "        h = self.sigmoid(X @ weights)\n",
        "        # Add epsilon to log arguments to prevent log(0)\n",
        "        epsilon = 1e-15\n",
        "        # Use binary cross-entropy\n",
        "        loss = (-1/m) * np.sum(y * np.log(h + epsilon) + (1-y) * np.log(1-h + epsilon))\n",
        "        return loss\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m, n = X.shape\n",
        "        self.weights = np.zeros(n)\n",
        "        self.loss_history = [] # Initialize loss history for the current fit\n",
        "\n",
        "        # Calculate initial loss before training\n",
        "        self.loss_history.append(self.compute_loss(X, y, self.weights))\n",
        "\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            # Calcular predicciones\n",
        "            h = self.sigmoid(X @ self.weights)\n",
        "\n",
        "            # Calcular gradiente\n",
        "            gradient = (1/m) * (X.T @ (h - y))\n",
        "\n",
        "            # Actualizar pesos\n",
        "            new_weights = self.weights - self.learning_rate * gradient\n",
        "\n",
        "            # Calculate loss after weight update (only if you want per-iteration loss)\n",
        "            # current_loss = self.compute_loss(X, y, new_weights)\n",
        "            # self.loss_history.append(current_loss)\n",
        "\n",
        "            # Verificar convergencia usando el gradiente\n",
        "            if np.linalg.norm(gradient) < self.tol:\n",
        "                 print(f\"Convergencia basada en gradiente alcanzada en la iteración {i+1}\")\n",
        "                 self.weights = new_weights # Update weights one last time\n",
        "                 # If loss history was not recorded per iteration, calculate final loss here\n",
        "                 # final_loss = self.compute_loss(X, y, self.weights)\n",
        "                 # self.loss_history.append(final_loss)\n",
        "                 break\n",
        "\n",
        "            self.weights = new_weights\n",
        "\n",
        "        # Always record the final loss after the loop finishes (either by max_iter or convergence)\n",
        "        final_loss = self.compute_loss(X, y, self.weights)\n",
        "        self.loss_history.append(final_loss) # Ensure final loss is recorded\n",
        "\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # Ensure input X has the correct shape (including intercept)\n",
        "        if X.shape[1] != self.weights.shape[0]:\n",
        "             raise ValueError(f\"Input shape mismatch in predict_proba: Expected {self.weights.shape[0]} features (including intercept), but got {X.shape[1]}.\")\n",
        "        return self.sigmoid(X @ self.weights)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return (self.predict_proba(X) >= threshold).astype(int)\n",
        "\n",
        "class MultiClassLogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-4):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.classifiers = []\n",
        "        self.classes = None # Initialize classes here\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.classifiers = [] # Clear classifiers list before refitting\n",
        "\n",
        "        # Ensure input X has the correct shape (including intercept)\n",
        "        # All binary classifiers will be trained on X with intercept\n",
        "        expected_features = X.shape[1]\n",
        "\n",
        "\n",
        "        for cls in self.classes:\n",
        "            print(f\"Training classifier for class: {cls}\") # Debug print\n",
        "\n",
        "            # Crear etiquetas binarias para la clase actual\n",
        "            y_binary = (y == cls).astype(int)\n",
        "\n",
        "            # Entrenar clasificador binario\n",
        "            lr = LogisticRegression(learning_rate=self.learning_rate,\n",
        "                                  max_iter=self.max_iter,\n",
        "                                  tol=self.tol)\n",
        "            lr.fit(X, y_binary) # X here already includes the intercept\n",
        "\n",
        "            # Verify fitted weights shape\n",
        "            if lr.weights.shape[0] != expected_features:\n",
        "                raise ValueError(f\"Fitted weights shape mismatch for class {cls}: Expected {expected_features}, but got {lr.weights.shape[0]}.\")\n",
        "\n",
        "            self.classifiers.append(lr)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # Ensure input X has the correct shape (including intercept)\n",
        "        if not self.classifiers:\n",
        "             raise RuntimeError(\"Model has not been fitted yet.\")\n",
        "        if X.shape[1] != self.classifiers[0].weights.shape[0]:\n",
        "             raise ValueError(f\"Input shape mismatch in predict_proba: Expected {self.classifiers[0].weights.shape[0]} features (including intercept), but got {X.shape[1]}.\")\n",
        "\n",
        "        probas = np.array([clf.predict_proba(X) for clf in self.classifiers]).T\n",
        "        # Normalizar para que sumen 1 (important for multi-class interpretation)\n",
        "        probas_sum = probas.sum(axis=1, keepdims=True)\n",
        "        # Handle case where sum is zero to avoid division by zero\n",
        "        probas_sum[probas_sum == 0] = 1 # Or a small epsilon\n",
        "        return probas / probas_sum\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        probas = self.predict_proba(X)\n",
        "        # Ensure classes are defined before using them as index\n",
        "        if self.classes is None:\n",
        "            raise RuntimeError(\"Model has not been fitted yet. Classes are not defined.\")\n",
        "        return self.classes[np.argmax(probas, axis=1)]\n",
        "\n",
        "# Crear y entrenar el modelo\n",
        "# The model expects X_train which already includes the intercept and is scaled\n",
        "model = MultiClassLogisticRegression(learning_rate=0.1, max_iter=1000)\n",
        "model.fit(X_train, y_train) # Use X_train with intercept and scaled features\n",
        "\n",
        "# --- Generate and Save Plots ---\n",
        "\n",
        "# Graficar la pérdida durante el entrenamiento (para el primer clasificador)\n",
        "print(\"\\nGenerating and Saving Loss Plot...\")\n",
        "if model.classifiers and model.classifiers[0].loss_history:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    # Only plot if loss history was recorded.\n",
        "    plt.plot(model.classifiers[0].loss_history)\n",
        "    plt.title('Pérdida durante el entrenamiento (Primer clasificador)')\n",
        "    plt.xlabel('Iteración')\n",
        "    plt.ylabel('Pérdida')\n",
        "    plt.grid()\n",
        "    plt.savefig('loss_plot.png') # Save the plot for Gradio\n",
        "    plt.show() # Show in notebook\n",
        "else:\n",
        "    print(\"No loss history recorded or classifiers not trained for loss plot. Skipping save and display.\")\n",
        "plt.close('all') # Close all figures to free memory\n",
        "\n",
        "# Calculate metrics before generating plots\n",
        "# Predecir en el conjunto de prueba\n",
        "# model.predict expects X_test which already includes the intercept and is scaled\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calcular métricas\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nPrecisión: {accuracy:.4f}\")\n",
        "\n",
        "# Matriz de confusión\n",
        "print(\"\\nMatriz de confusión:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# Reporte de clasificación\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Matriz de confusión visual\n",
        "print(\"\\nGenerating and Saving Confusion Matrix Plot...\")\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=model.classes, yticklabels=model.classes)\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.xlabel('Predicho')\n",
        "plt.ylabel('Real')\n",
        "plt.savefig('confusion_matrix_plot.png') # Save the plot for Gradio\n",
        "plt.show() # Show in notebook\n",
        "plt.close('all') # Close all figures to free memory\n",
        "\n",
        "\n",
        "# Curvas ROC (para clasificación binaria)\n",
        "print(\"\\nGenerating and Saving ROC Curve (if binary)...\")\n",
        "if len(model.classes) == 2:\n",
        "    # y_proba needs to be probabilities for the positive class (usually class 1)\n",
        "    # Find the index for class 1\n",
        "    class_1_idx = np.where(model.classes == 1)[0]\n",
        "    if class_1_idx.size > 0:\n",
        "        class_1_idx = class_1_idx[0]\n",
        "        y_proba = model.predict_proba(X_test)[:, class_1_idx] # Get probabilities for class 1\n",
        "        fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Curva ROC')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.savefig('roc_curve_plot.png') # Save the plot for Gradio\n",
        "        plt.show() # Show in notebook\n",
        "    else:\n",
        "        print(\"Class 1 not found in model classes for ROC curve calculation. Skipping ROC plot save and display.\")\n",
        "else:\n",
        "    print(\"Skipping ROC curve: Not a binary classification problem.\")\n",
        "plt.close('all') # Close all figures to free memory\n",
        "\n",
        "\n",
        "# Mostrar los coeficientes más importantes for print, not plot in Gradio\n",
        "print(\"\\nShowing top coefficients per class (printed to console)...\")\n",
        "# Ensure feature_names includes the intercept and excludes 'ID'\n",
        "feature_names_with_intercept = ['Intercept'] + list(data.drop(['default.payment.next.month', 'ID'], axis=1).columns)\n",
        "\n",
        "for i, cls in enumerate(model.classes):\n",
        "    print(f\"\\nCoeficientes para la clase {cls}:\")\n",
        "    if i < len(model.classifiers): # Check if classifier exists for this index\n",
        "        coef_df = pd.DataFrame({\n",
        "            'Feature': feature_names_with_intercept, # Use the feature names with intercept\n",
        "            'Coefficient': model.classifiers[i].weights # Access weights from the specific classifier\n",
        "        })\n",
        "        # Sort by absolute value of coefficient\n",
        "        coef_df['Abs_Coefficient'] = coef_df['Coefficient'].abs()\n",
        "        coef_df = coef_df.sort_values(by='Abs_Coefficient', ascending=False).drop('Abs_Coefficient', axis=1)\n",
        "\n",
        "        print(coef_df.head(10))\n",
        "    else:\n",
        "        print(f\"Classifier for class {cls} not found.\")\n",
        "\n",
        "\n",
        "# Guardar el modelo entrenado\n",
        "with open('credit_card_default_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Guardar el scaler\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "print(\"\\nModel and Scaler saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQJb39lFaHYh"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy scikit-learn gradio matplotlib seaborn\n",
        "\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os # Import os module to check for files\n",
        "\n",
        "# 1. Cargar modelo y scaler\n",
        "try:\n",
        "    with open('credit_card_default_model.pkl', 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "\n",
        "    with open('scaler.pkl', 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "\n",
        "    model_load_status = \"Modelo y Scaler cargados correctamente.\"\n",
        "except FileNotFoundError:\n",
        "     model_load_status = \"Error: Asegúrate de ejecutar la primera celda para entrenar y guardar el modelo y scaler.\"\n",
        "     model = None # Set model to None if loading fails\n",
        "     scaler = None # Set scaler to None if loading fails\n",
        "\n",
        "\n",
        "# 2. Verificación crítica (only if model loaded successfully)\n",
        "if model:\n",
        "    try:\n",
        "        print(\"Features esperadas por el scaler (sin intercepto):\", scaler.n_features_in_)  # Debe ser 23\n",
        "        print(\"Features entrenadas por el modelo (con intercepto):\", model.classifiers[0].weights.shape[0]) # Debe ser 24\n",
        "    except Exception as e:\n",
        "        print(f\"Error durante la verificación del modelo/scaler: {e}\")\n",
        "\n",
        "\n",
        "# 3. Definir TODAS las características (24 features)\n",
        "# Nota: Agregué 'INTERCEPT' como primer feature para coincidir con el scaler y el modelo entrenado\n",
        "feature_names = [\n",
        "    'INTERCEPT',  # ¡Nueva! Esto explica los 24 features\n",
        "    'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE',\n",
        "    'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',\n",
        "    'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4',\n",
        "    'BILL_AMT5', 'BILL_AMT6',\n",
        "    'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4',\n",
        "    'PAY_AMT5', 'PAY_AMT6'\n",
        "]\n",
        "\n",
        "# 4. Función para preparar inputs (8 → 24)\n",
        "def prepare_input(*input_values):\n",
        "    \"\"\"Convierte 8 inputs en array de 24 features (con intercepción)\"\"\"\n",
        "    # Valores por defecto para los features no proporcionados (ajustar según necesidad)\n",
        "    # Es crucial que estos valores por defecto estén en el mismo rango o escala que los datos de entrenamiento\n",
        "    # antes de la estandarización, aunque luego se escalarán. Usar 0 para los pagos/facturas\n",
        "    # retrasados no parece incorrecto si representa \"sin retraso/factura\".\n",
        "    default_values = {\n",
        "        'PAY_2': 0, 'PAY_3': 0, 'PAY_4': 0, 'PAY_5': 0, 'PAY_6': 0,\n",
        "        'BILL_AMT2': 0, 'BILL_AMT3': 0, 'BILL_AMT4': 0, 'BILL_AMT5': 0, 'BILL_AMT6': 0,\n",
        "        'PAY_AMT2': 0, 'PAY_AMT3': 0, 'PAY_AMT4': 0, 'PAY_AMT5': 0, 'PAY_AMT6': 0\n",
        "    }\n",
        "\n",
        "    # Mapear inputs proporcionados a sus nombres de característica\n",
        "    input_dict = {\n",
        "        'LIMIT_BAL': input_values[0],\n",
        "        'SEX': input_values[1],\n",
        "        'EDUCATION': input_values[2],\n",
        "        'MARRIAGE': input_values[3],\n",
        "        'AGE': input_values[4],\n",
        "        'PAY_0': input_values[5], # Assuming PAY_0 is the 'ÚLTIMO PAGO' input\n",
        "        'BILL_AMT1': input_values[6], # Assuming BILL_AMT1 is the 'MONTO FACTURA' input\n",
        "        'PAY_AMT1': input_values[7] # Assuming PAY_AMT1 is the 'ÚLTIMO PAGO REALIZADO' input\n",
        "    }\n",
        "\n",
        "    # Combinar inputs proporcionados con valores por defecto\n",
        "    full_features = {**default_values, **input_dict}\n",
        "\n",
        "    # Crear el array de features en el orden correcto, incluyendo el intercepto (1.0)\n",
        "    # Usamos feature_names para asegurar el orden\n",
        "    input_array = np.array([1.0] + [full_features[col] for col in feature_names[1:]])\n",
        "\n",
        "    return input_array\n",
        "\n",
        "# 5. Función de predicción\n",
        "def predict_credit_default(*input_values):\n",
        "    # Check if model and scaler were loaded successfully\n",
        "    if model is None or scaler is None:\n",
        "        return model_load_status, None # Return the error message\n",
        "\n",
        "    try:\n",
        "        # 1. Preparar input (array de 24 features, con intercepto incluido)\n",
        "        prepared_input = prepare_input(*input_values).reshape(1, -1)\n",
        "        # print(\"Input shape después de prepare_input:\", prepared_input.shape) # Debug print (shape should be (1, 24))\n",
        "\n",
        "        # 2. Estandarizar las características (excluir el intercepto en la columna 0)\n",
        "        # El scaler fue entrenado en los datos SIN intercepto.\n",
        "        # Por lo tanto, debemos aplicar la transformación solo a las columnas 1 en adelante.\n",
        "        prepared_input_scaled = prepared_input.copy() # Crear una copia para no modificar el original antes de escalar\n",
        "        prepared_input_scaled[:, 1:] = scaler.transform(prepared_input[:, 1:])\n",
        "        # print(\"Input shape después de escalar:\", prepared_input_scaled.shape) # Debug print (shape should be (1, 24))\n",
        "\n",
        "        # 3. Predecir\n",
        "        # El modelo espera el array con el intercepto ya incluido y las demás features escaladas.\n",
        "        prediction = model.predict(prepared_input_scaled)[0]\n",
        "        proba = model.predict_proba(prepared_input_scaled)[0]\n",
        "\n",
        "        # --- DEBUG PRINT: Check if probabilities change ---\n",
        "        print(f\"Prediction Probabilities: {proba}\")\n",
        "        # --------------------------------------------------\n",
        "\n",
        "        # 4. Visualización de probabilidad para la predicción actual\n",
        "        print(\"Generating individual prediction plot...\") # Debug print before plotting\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        # Ensure classes exist and are used correctly for plotting\n",
        "        if model.classes is not None and len(model.classes) == len(proba):\n",
        "             ax.bar(model.classes.astype(str), proba, color=['green', 'red']) # Usar las clases reales\n",
        "             ax.set_xticks(model.classes) # Asegurar que los ticks del eje x coincidan con las clases\n",
        "             ax.set_xticklabels(['No Default', 'Default']) # Etiquetas legibles\n",
        "             ax.set_ylabel(\"Probabilidad\")\n",
        "             ax.set_ylim(0, 1) # Limitar el eje Y entre 0 y 1\n",
        "        else:\n",
        "            ax.text(0.5, 0.5, \"Could not plot probabilities\", horizontalalignment='center', verticalalignment='center')\n",
        "\n",
        "        plt.close(fig)\n",
        "\n",
        "        # 5. Formato de salida\n",
        "        # Asegurarse de que las probabilidades corresponden a las clases 0 (No Default) y 1 (Default)\n",
        "        # model.classes[0] will be 0 and model.classes[1] will be 1 if they are ordered\n",
        "        # Use np.where to find indices just in case\n",
        "        proba_no_default = proba[np.where(model.classes == 0)[0][0]] if model.classes is not None and 0 in model.classes else 0\n",
        "        proba_default = proba[np.where(model.classes == 1)[0][0]] if model.classes is not None and 1 in model.classes else 0\n",
        "\n",
        "\n",
        "        return (\n",
        "            f\"Predicción: {'Default ⚠️' if prediction == 1 else 'No Default ✅'}\\n\"\n",
        "            f\"Probabilidad Default: {proba_default*100:.2f}%\\n\"\n",
        "            f\"Probabilidad No Default: {proba_no_default*100:.2f}%\",\n",
        "            fig\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(\"Error details:\", e) # Debug print\n",
        "        return f\"❌ Error durante la predicción: {str(e)}\", None\n",
        "\n",
        "# 6. Interfaz Gradio\n",
        "# Define the paths to the saved plot images\n",
        "LOSS_PLOT_PATH = 'loss_plot.png'\n",
        "CONFUSION_MATRIX_PLOT_PATH = 'confusion_matrix_plot.png'\n",
        "ROC_CURVE_PLOT_PATH = 'roc_curve_plot.png'\n",
        "\n",
        "# Check if plot files exist\n",
        "loss_plot_exists = os.path.exists(LOSS_PLOT_PATH)\n",
        "cm_plot_exists = os.path.exists(CONFUSION_MATRIX_PLOT_PATH)\n",
        "roc_plot_exists = os.path.exists(ROC_CURVE_PLOT_PATH)\n",
        "\n",
        "\n",
        "# Define inputs (same as before)\n",
        "inputs = [\n",
        "    gr.Number(label=\"LÍMITE DE CRÉDITO (LIMIT_BAL)\", value=200000),\n",
        "    gr.Dropdown(label=\"GÉNERO (SEX)\", choices=[(\"Masculino\", 1), (\"Femenino\", 2)], value=1),\n",
        "    gr.Dropdown(label=\"EDUCACIÓN (EDUCATION)\",\n",
        "               choices=[(\"Posgrado\", 1), (\"Universidad\", 2), (\"Preparatoria\", 3), (\"Otros\", 4), (\"(Desconocido)\", 0), (\"(Desconocido)\", 5), (\"(Desconocido)\", 6)], # Incluir todas las opciones posibles según el dataset\n",
        "               value=2),\n",
        "    gr.Dropdown(label=\"ESTADO CIVIL (MARRIAGE)\",\n",
        "               choices=[(\"Casado\", 1), (\"Soltero\", 2), (\"Otros\", 3), (\"(Desconocido)\", 0)], # Incluir todas las opciones posibles\n",
        "               value=1),\n",
        "    gr.Number(label=\"EDAD (AGE)\", value=30),\n",
        "    gr.Dropdown(label=\"ESTADO DE PAGO (Último Mes - PAY_0)\",\n",
        "               choices=[(\"Puntual\", 0), (\"Retraso 1 mes\", 1), (\"Retraso 2 meses\", 2),\n",
        "                        (\"Retraso 3 meses\", 3), (\"Retraso 4 meses\", 4),\n",
        "                        (\"Retraso 5 meses\", 5), (\"Retraso 6 meses\", 6),\n",
        "                        (\"Retraso 7 meses\", 7), (\"Retraso 8 meses\", 8),\n",
        "                        (\"Retraso 9+ meses\", 9), (\"Adelantado\", -1), (\"Adelantado\", -2)], # Incluir todas las opciones posibles\n",
        "               value=0),\n",
        "    gr.Number(label=\"MONTO FACTURA (Último Mes - BILL_AMT1)\", value=5000),\n",
        "    gr.Number(label=\"ÚLTIMO PAGO REALIZADO (PAY_AMT1)\", value=2000),\n",
        "]\n",
        "\n",
        "# Define outputs - add Image components for the saved plots\n",
        "outputs = [\n",
        "    gr.Textbox(label=\"Resultado de Predicción\"),\n",
        "    gr.Plot(label=\"Probabilidad de Default (Predicción Individual)\"),\n",
        "    # Add Image outputs for training/evaluation plots\n",
        "    gr.Image(label=\"Pérdida durante el Entrenamiento\", visible=loss_plot_exists) if loss_plot_exists else gr.Textbox(label=\"Pérdida durante el Entrenamiento\", value=\"Gráfica no disponible (ejecute la primera celda)\", visible=True),\n",
        "    gr.Image(label=\"Matriz de Confusión (Test Set)\", visible=cm_plot_exists) if cm_plot_exists else gr.Textbox(label=\"Matriz de Confusión (Test Set)\", value=\"Gráfica no disponible (ejecute la primera celda)\", visible=True),\n",
        "    gr.Image(label=\"Curva ROC (Test Set, si es Binario)\", visible=roc_plot_exists) if roc_plot_exists else gr.Textbox(label=\"Curva ROC (Test Set, si es Binario)\", value=\"Gráfica no disponible (ejecute la primera celda)\", visible=True)\n",
        "]\n",
        "\n",
        "# Update the fn function to also return the plot images/placeholders\n",
        "def full_interface_fn(*input_values):\n",
        "    # Call the prediction function first\n",
        "    prediction_output_text, prediction_output_plot = predict_credit_default(*input_values)\n",
        "\n",
        "    # Load the saved plot images if they exist\n",
        "    loss_plot_img = LOSS_PLOT_PATH if os.path.exists(LOSS_PLOT_PATH) else None\n",
        "    cm_plot_img = CONFUSION_MATRIX_PLOT_PATH if os.path.exists(CONFUSION_MATRIX_PLOT_PATH) else None\n",
        "    roc_plot_img = ROC_CURVE_PLOT_PATH if os.path.exists(ROC_CURVE_PLOT_PATH) else None\n",
        "\n",
        "    # Return all outputs in the order defined in the outputs list\n",
        "    # Need to return the prediction_output_plot here\n",
        "    return prediction_output_text, prediction_output_plot, loss_plot_img, cm_plot_img, roc_plot_img\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=full_interface_fn, # Use the new function that returns all outputs\n",
        "    inputs=inputs,\n",
        "    outputs=outputs, # Use the updated outputs list\n",
        "    title=\"🔍 Predictor de Riesgo Crediticio y Evaluación del Modelo\",\n",
        "    description=\"Complete los 8 campos clave para obtener una predicción individual. También se muestran gráficas de evaluación del modelo entrenado.\",\n",
        "    examples=[\n",
        "        [200000, 1, 2, 1, 30, 0, 5000, 2000],\n",
        "        [50000, 2, 3, 2, 45, 2, 25000, 500],\n",
        "        [10000, 1, 1, 1, 25, -1, 1000, 5000], # Example with early payment\n",
        "        [300000, 2, 2, 2, 50, 3, 100000, 0] # Example with significant delay\n",
        "    ]\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
